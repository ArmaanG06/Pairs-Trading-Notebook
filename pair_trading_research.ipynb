{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab251e0",
   "metadata": {},
   "source": [
    "# Pair Trading Strategy Research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3045d",
   "metadata": {},
   "source": [
    "Armaan Gandhara | agandhara243@gmail.com | armaangandhara.me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d443111",
   "metadata": {},
   "source": [
    "09/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271843de",
   "metadata": {},
   "source": [
    "## Config & Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2143b368",
   "metadata": {},
   "source": [
    "*Purpose* : Centeralized parameters, imports, styles, and small helpers reused across the notebook. This keeps later sections focused on research and backtesting logic, not boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e5919b",
   "metadata": {},
   "source": [
    "Whats inside:\n",
    "- Project config (Config dataclass): dates, universe, paths, risk-free, frequency\n",
    "- Reproducibility: seed setter\n",
    "- Plot style: consistent figures \n",
    "- Helpers: annualizer factor, returns, rolling z-score, drawdown and risk metrics, alignment utilities\n",
    "- Lightweight disk cache utility for later data ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b54e7",
   "metadata": {},
   "source": [
    "### Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8c0195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    start=\"2017-01-01\",\n",
    "    end = \"2025-09-01\",\n",
    "    tickers = [\"MSFT\", \"AAPL\", \"GOOGL\", \"AMZN\", \"META\"],\n",
    "    data_dir=\"data\"\n",
    ")\n",
    "set_seed(42)\n",
    "set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab48848",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b75efb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Config & Utils\n",
    "# =======================\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import warnings\n",
    "from typing import Iterable, Tuple, Optional, Dict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Config ----------\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    start:str\n",
    "    end:str\n",
    "    tickers:Iterable[str]\n",
    "    data_dir: str = \"data\"\n",
    "    freq: str = \"D\"\n",
    "    trading_days: int = 252\n",
    "    rf_annual: float = 0.00\n",
    "\n",
    "    def path(self):\n",
    "        p = Path(self.data_dir)\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        (p/\"cache\").mkdir(parents=True, exist_ok=True)\n",
    "        return p\n",
    "    \n",
    "# ---------- Repro/Style ----------\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def set_plot_style():\n",
    "    plt.rcParams.update({\n",
    "        \"figure.figsize\": (10,5),\n",
    "        \"axes.grid\":True,\n",
    "        \"grid.alpha\": 0.3,\n",
    "        \"font.size\": 11,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "    })\n",
    "\n",
    "# ---------- Frequencies/Annualization ----------\n",
    "\n",
    "_ANNUALIZE = {\n",
    "    \"D\": 252,\n",
    "    \"B\": 252,\n",
    "    \"W\": 52,\n",
    "    \"M\": 12,\n",
    "}\n",
    "\n",
    "def annualization_factor(freq:str):\n",
    "    return _ANNUALIZE.get(freq.upper(),252)\n",
    "\n",
    "# ---------- Returns & Z-Score ----------\n",
    "\n",
    "def compute_returns(prices:pd.DataFrame, method:str=\"log\"):\n",
    "    \"\"\"\n",
    "    Compute log or simple returns from price levels\n",
    "    \"\"\"\n",
    "    if method not in {\"log\", \"simple\"}:\n",
    "        raise ValueError(\"method must be 'log' or 'simple'\")\n",
    "    px = prices.sort_index()\n",
    "    if method == \"log\":\n",
    "        rets = np.log(px).diff()\n",
    "    else:\n",
    "        rets   = px.pct_change()\n",
    "    return rets.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def zscore_rolling(x: pd.Series, window:int):\n",
    "    mu = x.rolling(window).mean()\n",
    "    sigma = x.rolling(window).std(ddof=0)\n",
    "    z = (x - mu) / sigma\n",
    "    return z\n",
    "    \n",
    "\n",
    "# ---------- Drawdowns & Risk Metrics ----------\n",
    "\n",
    "def equity_to_drawdown(equity:pd.Series):\n",
    "    cummax = equity.cummax()\n",
    "    dd = equity/cummax - 1.0\n",
    "    return dd\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series, freq:str=\"D\", rf_annual: float = 0.0):\n",
    "    af = annualization_factor(freq)\n",
    "    rf_per_step = (1+rf_annual)**(1/af) - 1\n",
    "    ex = returns - rf_per_step\n",
    "    mu = ex.mean() * af\n",
    "    sigma = ex.std(ddof=0) * np.sqrt(af)\n",
    "    if sigma == 0 or np.isnan(sigma):\n",
    "        a = np.nan\n",
    "    else:\n",
    "        a = mu/sigma\n",
    "    return a\n",
    "\n",
    "def sortino_ratio(returns:pd.Series, freq:str = \"D\", rf_annual:float = 0.0):\n",
    "    af = annualization_factor(freq)\n",
    "    rf_per_step = (1 + rf_annual) ** (1/af) - 1\n",
    "    ex = returns - rf_per_step\n",
    "    downside = ex.clip(upper = 0)\n",
    "    dd_sigma = downside.std(ddof=0) * np.sqrt(af)\n",
    "    mu = ex.mean() * af\n",
    "    if dd_sigma == 0 or np.isnan(dd_sigma):\n",
    "        a = np.nan\n",
    "    else:\n",
    "        a = mu/ dd_sigma\n",
    "    return a\n",
    "    \n",
    "def calmar_ratio(equity: pd.Series, freq: str = \"D\"):\n",
    "    af = annualization_factor(freq)\n",
    "    rets = equity.pct_change().dropna()\n",
    "    cagr = (equity.dropna().iloc[-1] / equity.dropna().iloc[0]) ** (af / len(rets)) - 1\n",
    "    dd = equity_to_drawdown(equity).min()\n",
    "    max_dd = abs(dd) if pd.notna(dd) else np.nan\n",
    "    if not max_dd or max_dd == 0:\n",
    "        a = np.nan\n",
    "    else:\n",
    "        a = cagr/max_dd\n",
    "    return a\n",
    "\n",
    "def max_drawdown(equity:pd.Series):\n",
    "    return abs(equity_to_drawdown(equity).min())\n",
    "\n",
    "# ---------- Alignment/Cleaning ----------\n",
    "\n",
    "def ensure_datetime_index(df: pd.DataFrame):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df = df.copy()\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    return df.sort_index()\n",
    "\n",
    "def align_panels(*dfs: pd.DataFrame, dropna: bool = True):\n",
    "    \"\"\"\n",
    "    Align multiple DataFrames on the intersection of dates and shared columns.\n",
    "    \"\"\"\n",
    "    cols = set(dfs[0].columns)                    \n",
    "    for d in dfs[1:]:\n",
    "        cols &= set(d.columns)\n",
    "    cols = sorted(list(cols))\n",
    "    aligned = [ensure_datetime_index(d)[cols] for d in dfs]\n",
    "    idx = aligned[0].index\n",
    "    for d in aligned[1:]:\n",
    "        idx = idx.intersection(d.index)\n",
    "    aligned = [a.loc[idx] for a in aligned]\n",
    "    if dropna:\n",
    "        good = ~pd.concat([a.isna().any(axis=1) for a in aligned], axis=1).any(axis=1)\n",
    "        aligned = [a.loc[good] for a in aligned]\n",
    "    return tuple(aligned)\n",
    "\n",
    "# ---------- Lightweight Disk Cache (CSV version) ----------\n",
    "\n",
    "def _hash_key(key: Dict):\n",
    "    payload = json.dumps(key, sort_keys=True).encode()\n",
    "    return hashlib.md5(payload).hexdigest()\n",
    "\n",
    "def cache_path(base_dir: Path, namespace: str, key: Dict):\n",
    "    h = _hash_key(key)\n",
    "    p = base_dir / \"cache\" / namespace\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p / f\"{h}.csv\"\n",
    "\n",
    "def cache_save(df: pd.DataFrame, path: Path):\n",
    "    df.to_csv(path, index=True)\n",
    "\n",
    "def cache_load(path: Path):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            return pd.read_csv(path, index_col=0, parse_dates=True)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# --- IBFetcher integration (file-based import) ---\n",
    "\n",
    "import importlib.util, sys\n",
    "\n",
    "def _load_ibfetcher_module(root: str = \"external/IBFetcher\", filename: str = \"ibfetcher.py\"):\n",
    "    path = Path(root) / filename\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"IBFetcher not found at {path.resolve()}\")\n",
    "    spec = importlib.util.spec_from_file_location(\"ibfetcher\", path)\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"ibfetcher\"] = mod\n",
    "    spec.loader.exec_module(mod)  # type: ignore[attr-defined]\n",
    "    return mod\n",
    "\n",
    "def _ib_duration_from_dates(start: str, end: str) -> str:\n",
    "    # IB expects strings like \"300 D\" or \"2 Y\". Use days; IB will handle.\n",
    "    days = (pd.to_datetime(end) - pd.to_datetime(start)).days\n",
    "    days = max(1, int(days))\n",
    "    return f\"{days} D\"\n",
    "\n",
    "def fetch_prices_ib(tickers: Iterable[str],\n",
    "                    start: str,\n",
    "                    end: str,\n",
    "                    ib_root: str = \"external/IBFetcher\",\n",
    "                    bar_size: str = \"1 day\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use your IBFetcher class to pull OHLCV for each ticker and return:\n",
    "    columns = MultiIndex {'close','volume'} x tickers, DateTimeIndex in UTC-naive.\n",
    "    \"\"\"\n",
    "    ibm = _load_ibfetcher_module(ib_root, \"ibfetcher.py\")\n",
    "    IBFetcher = getattr(ibm, \"IBFetcher\")\n",
    "    ib = IBFetcher()\n",
    "\n",
    "    # connect TWS or IB Gateway first (adjust host/port/client_id if needed)\n",
    "    ib.connect_app(host=\"127.0.0.1\", port=7496, client_id=1)\n",
    "\n",
    "    duration = _ib_duration_from_dates(start, end)\n",
    "    tks = sorted({t.upper().replace(\".\", \"-\").strip() for t in tickers})\n",
    "\n",
    "    frames = []\n",
    "    for t in tks:\n",
    "        df = ib.fetch_stock_data(\n",
    "            symbol=t, duration=duration, bar_size=bar_size, endDateTime=\"\"\n",
    "        )\n",
    "        if df is None or df.empty:\n",
    "            raise ValueError(f\"No data returned for {t}\")\n",
    "        # expected columns: ['datetime','open','high','low','close','volume']\n",
    "        df = df.copy()\n",
    "        # normalize index\n",
    "        if \"datetime\" in df.columns:\n",
    "            df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "            df = df.set_index(\"datetime\")\n",
    "        else:\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        df = df.sort_index()\n",
    "        # keep close and volume\n",
    "        need = []\n",
    "        if \"close\" in df.columns: need.append(\"close\")\n",
    "        if \"volume\" in df.columns: need.append(\"volume\")\n",
    "        if len(need) < 2:\n",
    "            raise ValueError(f\"{t}: expected close and volume. Got {list(df.columns)}\")\n",
    "        out = df[need].copy()\n",
    "        out.columns = pd.MultiIndex.from_product([need, [t]])\n",
    "        frames.append(out)\n",
    "\n",
    "    ib.disconnect_app()\n",
    "\n",
    "    out = pd.concat(frames, axis=1).sort_index()\n",
    "    # full structure and date filtering\n",
    "    full_cols = pd.MultiIndex.from_product([[\"close\",\"volume\"], tks])\n",
    "    out = out.reindex(columns=full_cols)\n",
    "    out = out.loc[(out.index >= pd.to_datetime(start)) & (out.index <= pd.to_datetime(end))]\n",
    "    return ensure_datetime_index(out)\n",
    "\n",
    "# --- Cache-first loader using IB, with CSV cache in data/cache/prices ---\n",
    "\n",
    "def _norm_tickers(tickers: Iterable[str]):\n",
    "    return sorted({t.upper().replace(\".\", \"-\").strip() for t in tickers})\n",
    "\n",
    "def _prices_cache_key(tickers: Iterable[str], start: str, end: str) -> Dict:\n",
    "    return {\"tickers\": _norm_tickers(tickers), \"start\": str(start), \"end\": str(end)}\n",
    "\n",
    "def load_prices(cfg: Config,\n",
    "                ib_root: str = \"external/IBFetcher\",\n",
    "                bar_size: str = \"1 day\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Try project CSV cache (data/cache/prices/<hash>.csv).\n",
    "    2) If miss, call IBFetcher and save to cache.\n",
    "    \"\"\"\n",
    "    key = _prices_cache_key(cfg.tickers, cfg.start, cfg.end)\n",
    "    fpath = cache_path(cfg.path(), \"prices\", key)\n",
    "\n",
    "    df = cache_load(fpath)\n",
    "    if df is not None and isinstance(df.columns, pd.MultiIndex):\n",
    "        return ensure_datetime_index(df)\n",
    "\n",
    "    df = fetch_prices_ib(cfg.tickers, cfg.start, cfg.end, ib_root=ib_root, bar_size=bar_size)\n",
    "    cache_save(df, fpath)\n",
    "    return ensure_datetime_index(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05436cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to IB...\n",
      "Error 502: Couldn't connect to TWS. Confirm that \"Enable ActiveX and Socket EClients\" \n",
      "is enabled and connection port is the same as \"Socket Port\" on the \n",
      "TWS \"Edit->Global Configuration...->API->Settings\" menu. Live Trading ports: \n",
      "TWS: 7496; IB Gateway: 4001. Simulated Trading ports for new installations \n",
      "of version 954.1 or newer:  TWS: 7497; IB Gateway: 4002\n",
      "Connected.\n",
      "Error 504: Not connected\n"
     ]
    }
   ],
   "source": [
    "cfg = Config(\n",
    "    start=\"2019-01-01\",\n",
    "    end=\"2025-09-01\",\n",
    "    tickers=[\"MSFT\",\"AAPL\",\"GOOGL\"],\n",
    "    data_dir=\"data\"\n",
    ")\n",
    "set_seed(42); set_plot_style()\n",
    "\n",
    "prices = load_prices(cfg, ib_root=\"external/IBFetcher\", bar_size=\"1 day\")\n",
    "prices.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381f762",
   "metadata": {},
   "source": [
    "## Data Ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a257cec",
   "metadata": {},
   "source": [
    "## Pair Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef44e0b",
   "metadata": {},
   "source": [
    "## Hedge Ratio & Spread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c41d06",
   "metadata": {},
   "source": [
    "## OU Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7de650",
   "metadata": {},
   "source": [
    "## Signals & Sizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8bee3",
   "metadata": {},
   "source": [
    "## Cost and Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14baa37",
   "metadata": {},
   "source": [
    "## Walk-Forard Backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0993f69",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08a404",
   "metadata": {},
   "source": [
    "## Factor Neautrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c784f",
   "metadata": {},
   "source": [
    "## Sensitivity Sweeps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f22b3",
   "metadata": {},
   "source": [
    "## Regime Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829a919",
   "metadata": {},
   "source": [
    "## OOS Holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1dc23c",
   "metadata": {},
   "source": [
    "## Beta Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063198fd",
   "metadata": {},
   "source": [
    "## Structural Breaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a56ed4",
   "metadata": {},
   "source": [
    "## Cost Stress Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed2275",
   "metadata": {},
   "source": [
    "## ML Ranker for Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9205c99",
   "metadata": {},
   "source": [
    "## Intraday Extensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
